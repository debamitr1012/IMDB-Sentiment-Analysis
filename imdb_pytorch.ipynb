{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61693ddb",
   "metadata": {},
   "source": [
    "# IMDB with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a030d2f",
   "metadata": {},
   "source": [
    "## IMDB Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de869e",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf3519",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3bf3519",
    "outputId": "fa8dc99b-35c1-403e-d92b-0ae3de23bae0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078b66e",
   "metadata": {},
   "source": [
    "# \n",
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f61de5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "25f61de5",
    "outputId": "dfb885a2-14d5-481a-a5a7-ed217839288f"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\IMDB Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de535fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "5de535fd",
    "outputId": "ee2b0a3e-93f4-4753-8cd9-3f891a9fe32f"
   },
   "outputs": [],
   "source": [
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "data['label'] = data['sentiment'].progress_apply(transform_label)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52cb76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb52cb76",
    "outputId": "8360889e-3839-4696-96ec-26ba5e75f220"
   },
   "outputs": [],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93062e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b93062e",
    "outputId": "65493a4a-d1a7-40ee-cc04-deca12762fc2"
   },
   "outputs": [],
   "source": [
    "data['token_length'] = data.review.progress_apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35e7cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e35e7cc",
    "outputId": "44794364-1ebc-4104-ccd1-0f67e962a396"
   },
   "outputs": [],
   "source": [
    "data_pos = data[data['label'] == 1]\n",
    "data_pos['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bc86b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a17bc86b",
    "outputId": "d4db7c71-b2d7-4140-a190-3febf2b92e2e"
   },
   "outputs": [],
   "source": [
    "data_neg = data[data['label'] == 0]\n",
    "data_neg['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bca712",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "36bca712",
    "outputId": "d9d699c9-0093-4ed9-e835-07c10fe7b8b1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "sns.displot(data_pos, x='token_length')\n",
    "plt.title('Positive Token Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0696e6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "a0696e6d",
    "outputId": "89772dde-9914-48c1-a1c9-e28d3682cfe7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "sns.displot(data_pos, x='token_length')\n",
    "plt.title('Negative Token Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696d529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5696d529",
    "outputId": "94fe01f5-3603-4dee-cf61-df3ea3755870"
   },
   "outputs": [],
   "source": [
    "print('Positive')\n",
    "print(data_pos[data_pos['token_length'] == data_pos['token_length'].min()]['review'].item())\n",
    "print()\n",
    "print('Negative')\n",
    "print(data_neg[data_neg['token_length'] == data_neg['token_length'].min()]['review'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50492bcf",
   "metadata": {},
   "source": [
    "# \n",
    "Clean and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088007b",
   "metadata": {
    "id": "a088007b"
   },
   "outputs": [],
   "source": [
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "def rm_punct2(text):\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "def rm_html(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "def space_bt_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)     \n",
    "    s = re.sub(r'\\s{2,}', ' ', s)         \n",
    "    return s\n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  \n",
    "        u'\\U0001F300-\\U0001F5FF'  \n",
    "        u'\\U0001F680-\\U0001F6FF'  \n",
    "        u'\\U0001F1E0-\\U0001F1FF'  \n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "def spell_correction(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "def clean_pipeline(text):    \n",
    "    no_link = rm_link(text)\n",
    "    no_html = rm_html(no_link)\n",
    "    space_punct = space_bt_punct(no_html)\n",
    "    no_punct = rm_punct2(space_punct)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonasci = rm_nonascii(no_whitespaces)\n",
    "    no_emoji = rm_emoji(no_nonasci)\n",
    "    spell_corrected = spell_correction(no_emoji)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c988f90",
   "metadata": {
    "id": "1c988f90"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "def rm_stopwords(text):\n",
    "    return [i for i in text if i not in stopwords]\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    return rm_stopwords(lemmas)\n",
    "def preprocess_pipeline(text):\n",
    "    tokens = tokenize(text)\n",
    "    no_stopwords = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9gDmO9D8lk_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9gDmO9D8lk_",
    "outputId": "59daa72d-3091-4833-923f-5db7d499e53a"
   },
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b22c4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "51b22c4e",
    "outputId": "5c2fcf1a-fc0d-49fa-d93c-13cd994344bb"
   },
   "outputs": [],
   "source": [
    "data['clean'] = data['review'].progress_apply(clean_pipeline)\n",
    "data['processed'] = data['clean'].progress_apply(preprocess_pipeline)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63942032",
   "metadata": {
    "id": "63942032"
   },
   "outputs": [],
   "source": [
    "data[['processed', 'label']].to_csv('C:\\IMDB Dataset.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7463bd",
   "metadata": {},
   "source": [
    "# \n",
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec2565",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01ec2565",
    "outputId": "20b73cc1-1371-4c28-e4fc-cef3586e7b38"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\IMDB Dataset.csv')\n",
    "for row in data[:2].iterrows():\n",
    "    print(row[1]['processed'])\n",
    "    print(f'Label: {row[1][\"label\"]}')    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b247a1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b247a1f",
    "outputId": "de15c759-14d5-4661-dded-085119150b9d"
   },
   "outputs": [],
   "source": [
    "reviews = data.processed.values\n",
    "words = ' '.join(reviews)\n",
    "words = words.split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41332d34",
   "metadata": {
    "id": "41332d34"
   },
   "outputs": [],
   "source": [
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "int2word[0] = '<PAD>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8e3ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60f8e3ae",
    "outputId": "7ec48e0a-1606-4b23-e4d6-1b2cb0e530a2"
   },
   "outputs": [],
   "source": [
    "reviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n",
    "for i in range(5):\n",
    "    print(reviews_enc[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf0db1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82bf0db1",
    "outputId": "b0cf7dc6-0c3b-44d6-c223-2a0f3d7dfca0"
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "    for i, row in enumerate(reviews):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    return features\n",
    "seq_length = 256\n",
    "features = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)\n",
    "assert len(features) == len(reviews_enc)\n",
    "assert len(features[0]) == seq_length\n",
    "features[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf9a18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aaf9a18",
    "outputId": "62843d25-a9ef-4d61-f17e-70edfa5be495"
   },
   "outputs": [],
   "source": [
    "labels = data.label.to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0999c44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0999c44",
    "outputId": "699cdbcb-4a29-4aca-eefa-62a88d3711b5"
   },
   "outputs": [],
   "source": [
    "train_size = .7    \n",
    "val_size = .5      \n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, remain_x = features[:split_id], features[split_id:]\n",
    "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
    "split_val_id = int(len(remain_x) * val_size)\n",
    "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
    "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n",
    "print('Feature Shapes:')\n",
    "print('Train set: {}'.format(train_x.shape))\n",
    "print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220826cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "220826cf",
    "outputId": "57f86472-1461-425a-81b9-8cfcdd6317a8"
   },
   "outputs": [],
   "source": [
    "print(len(train_y[train_y == 0]), len(train_y[train_y == 1]))\n",
    "print(len(val_y[val_y == 0]), len(val_y[val_y == 1]))\n",
    "print(len(test_y[test_y == 0]), len(test_y[test_y == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870bb28",
   "metadata": {
    "id": "5870bb28"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "validset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "testset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "valloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n",
    "testloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e8b6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c50e8b6f",
    "outputId": "c5ea5b72-7f1a-492a-fc16-45a7a6682d15"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "x, y = dataiter.next()\n",
    "print('Sample batch size: ', x.size())   \n",
    "print('Sample batch input: \\n', x)\n",
    "print()\n",
    "print('Sample label size: ', y.size())  \n",
    "print('Sample label input: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c52c2",
   "metadata": {},
   "source": [
    "# \n",
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4ba14",
   "metadata": {
    "id": "80b4ba14"
   },
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size=128, embedding_size=400, n_layers=2, dropout=0.2):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        x = self.embedding(x)\n",
    "        o, _ =  self.lstm(x)\n",
    "        o = o[:, -1, :]\n",
    "        o = self.dropout(o)\n",
    "        o = self.fc(o)\n",
    "        o = self.sigmoid(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc33ead",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bc33ead",
    "outputId": "f4a38ae3-a533-4705-b1ce-8bfb75f7b5fe"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4300f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68a4300f",
    "outputId": "b832fd83-a32e-46cd-d300-9634e4d23469"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2int)\n",
    "output_size = 1\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "dropout=0.25\n",
    "model = SentimentModel(vocab_size, output_size, hidden_size, embedding_size, n_layers, dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdbef8",
   "metadata": {
    "id": "c7bdbef8"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion = nn.BCELoss()  \n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "grad_clip = 5\n",
    "epochs = 8\n",
    "print_every = 1\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'epochs': epochs\n",
    "}\n",
    "es_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783ddc3",
   "metadata": {},
   "source": [
    "# \n",
    "PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93eb7fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a93eb7fc",
    "outputId": "529c1e6a-f404-4821-aff6-242d60585300"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "es_trigger = 0\n",
    "val_loss_min = torch.inf\n",
    "for e in epochloop:\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for id, (feature, target) in enumerate(trainloader):\n",
    "        epochloop.set_postfix_str(f'Training batch {id}/{len(trainloader)}')\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "        optim.zero_grad()\n",
    "        out = model(feature)\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_acc += acc.item()\n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optim.step()\n",
    "        del feature, target, predicted\n",
    "    history['train_loss'].append(train_loss / len(trainloader))\n",
    "    history['train_acc'].append(train_acc / len(trainloader))\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for id, (feature, target) in enumerate(valloader):\n",
    "            epochloop.set_postfix_str(f'Validation batch {id}/{len(valloader)}')\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "            out = model(feature)\n",
    "            predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            val_acc += acc.item()\n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            val_loss += loss.item()\n",
    "            del feature, target, predicted\n",
    "        history['val_loss'].append(val_loss / len(valloader))\n",
    "        history['val_acc'].append(val_acc / len(valloader))\n",
    "    model.train()\n",
    "    epochloop.set_postfix_str(f'Val Loss: {val_loss / len(valloader):.3f} | Val Acc: {val_acc / len(valloader):.3f}')\n",
    "    if (e+1) % print_every == 0:\n",
    "        epochloop.write(f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} Train Acc: {train_acc / len(trainloader):.3f} | Val Loss: {val_loss / len(valloader):.3f} Val Acc: {val_acc / len(valloader):.3f}')\n",
    "        epochloop.update()\n",
    "    if val_loss / len(valloader) <= val_loss_min:\n",
    "        torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "        val_loss_min = val_loss / len(valloader)\n",
    "        es_trigger = 0\n",
    "    else:\n",
    "        epochloop.write(f'[WARNING] Validation loss did not improved ({val_loss_min:.3f} --> {val_loss / len(valloader):.3f})')\n",
    "        es_trigger += 1\n",
    "    if es_trigger >= es_limit:\n",
    "        epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "        history['epochs'] = e+1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a27926",
   "metadata": {
    "id": "e1a27926"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "plt.plot(range(history['epochs']), history['train_acc'], label='Train Acc')\n",
    "plt.plot(range(history['epochs']), history['val_acc'], label='Val Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4986949",
   "metadata": {
    "id": "d4986949"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "plt.plot(range(history['epochs']), history['train_loss'], label='Train Loss')\n",
    "plt.plot(range(history['epochs']), history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cd267",
   "metadata": {},
   "source": [
    "# \n",
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f18ed9",
   "metadata": {
    "id": "74f18ed9"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "all_target = []\n",
    "all_predicted = []\n",
    "testloop = tqdm(testloader, leave=True, desc='Inference')\n",
    "with torch.no_grad():\n",
    "    for feature, target in testloop:\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "        out = model(feature)\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        test_acc += acc.item()\n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        test_loss += loss.item()\n",
    "        all_target.extend(target.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "    print(f'Accuracy: {test_acc/len(testloader):.4f}, Loss: {test_loss/len(testloader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49388802",
   "metadata": {},
   "source": [
    "# \n",
    "Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27299f",
   "metadata": {
    "id": "8b27299f"
   },
   "outputs": [],
   "source": [
    "print(classification_report(all_predicted, all_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e980c3",
   "metadata": {
    "id": "98e980c3"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_predicted, all_target)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "imdb_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "88b2cf4e8d0b67568ad14b84a5c832fc32f25f2ffbc1f06c6b6ffdb5a46993f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
